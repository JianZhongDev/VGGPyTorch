{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818dc37d-4267-4e87-97e4-575c3b2a9af2",
   "metadata": {},
   "source": [
    "# VGG_Train_w_PreTrainModel \n",
    "DESCRIPTION: This notebook includes training and validation scripts for deep VGG model. \n",
    "\n",
    "Including transfering parameters from pretrained model to new model.\n",
    "\n",
    "@author: Jian Zhong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aaeaf2-4f69-4b39-bc31-5522232e3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import modules\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d2002-2d20-4106-8ed4-54b1767bed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import customized modules \n",
    "import Modules.Models.VGG as VGG\n",
    "from Modules.Data.Transforms import (\n",
    "    subtract_channel_mean, \n",
    "    subtract_const, \n",
    "    random_ch_shift_pca,\n",
    ")\n",
    "from Modules.TrainAndValidate.TrainValidate import (\n",
    "    train_one_epoch, \n",
    "    validate_one_epoch, \n",
    "    validate_one_epoch_topk, \n",
    "    validate_one_epoch_topk_aug,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc8447-f69b-4029-933e-c68c1d9178c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create customized transforms\n",
    "\n",
    "## subtract global mean channel background\n",
    "train_data_ch_avg = torch.tensor([[[0.4914]],[[0.4822]],[[0.4465]]])\n",
    "print(train_data_ch_avg.size())\n",
    "subtract_ch_avg = functools.partial(subtract_const, const_val = train_data_ch_avg)\n",
    "\n",
    "## random channel shifts\n",
    "trainset_pca_eigenvecs = torch.tensor([[-0.5580,  0.7063,  0.4356], [-0.5775,  0.0464, -0.8151], [-0.5960, -0.7063,  0.3820]])\n",
    "print(trainset_pca_eigenvecs.size())\n",
    "trainset_pca_eigenvals = torch.tensor([0.1719, 0.0139, 0.0029])\n",
    "print(trainset_pca_eigenvals.size())\n",
    "\n",
    "random_ch_shift = functools.partial(random_ch_shift_pca, \n",
    "                                    pca_eigenvecs = trainset_pca_eigenvecs,\n",
    "                                    pca_eigenvals = trainset_pca_eigenvals,\n",
    "                                    random_paras = {\"mean\": 0, \"std\": 0.1},\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1961ade-16e4-4802-93cf-3c751dc21cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define all data transform\n",
    "\n",
    "## data transform for training\n",
    "train_data_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32,scale = True),\n",
    "    v2.Lambda(subtract_ch_avg),\n",
    "    v2.RandomHorizontalFlip(0.5),\n",
    "    v2.Lambda(random_ch_shift),\n",
    "])\n",
    "\n",
    "## data transform for validation\n",
    "validate_data_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32,scale = True),\n",
    "    v2.Lambda(subtract_ch_avg),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd51ff-d5f1-4faa-bd6b-69f7370d46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data set\n",
    "\n",
    "## NOTE: the dataset_root_dir may need to be changed accordingly\n",
    "dataset_root_dir = r\"E:\\Python\\DataSet\\TorchDataSet\\CIFAR10\"\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    root = dataset_root_dir,\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform=train_data_transforms,\n",
    ")\n",
    "validate_data = torchvision.datasets.CIFAR10(\n",
    "    root = dataset_root_dir,\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform=validate_data_transforms,\n",
    ")\n",
    "\n",
    "print(f\"train_data length: {len(train_data)}\")\n",
    "print(f\"validate_data lenght: {len(validate_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd04e49-f94d-4ece-ae79-9058ba11aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data loader\n",
    "\n",
    "train_batch_size = 128\n",
    "validate_batch_size = 128\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, \n",
    "                                               batch_size = train_batch_size, \n",
    "                                               shuffle = True)\n",
    "validate_dataloader = torch.utils.data.DataLoader(validate_data, \n",
    "                                                  batch_size = validate_batch_size, \n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f1b0e-bfab-48a0-bf81-b49b43c72a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check data\n",
    "\n",
    "check_data_idx = 0\n",
    "\n",
    "check_dataloader = train_dataloader\n",
    "\n",
    "check_features, check_labels = next(iter(check_dataloader))\n",
    "\n",
    "print(f\"Feature batch shape: {check_features.size()}\")\n",
    "print(f\"Labels batch shape: {check_labels.size()}\")\n",
    "\n",
    "check_feature = check_features[check_data_idx].squeeze().numpy()\n",
    "check_image = (check_features[check_data_idx].squeeze()+train_data_ch_avg).numpy()\n",
    "check_label = check_labels[check_data_idx].numpy()\n",
    "\n",
    "plt.figure()\n",
    "for i_ch in range(check_feature.shape[0]):\n",
    "    plt.hist(check_feature[i_ch,...].reshape((-1,)), label = f\"ch{i_ch}\")\n",
    "plt.legend()\n",
    "plt.title(f\"feature channel histogram\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.rollaxis(check_image, 0, 3))\n",
    "plt.title(f\"transformed image, label = {check_label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8ff48-bbda-4d5f-b818-04c6f645afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define model\n",
    "\n",
    "## NOTE: input image width and height need to be adjusted based on data \n",
    "input_image_width = 32\n",
    "input_image_height = 32\n",
    "\n",
    "model_stacked_conv_list = [\n",
    "    [ \n",
    "        {\"nof_layers\": 1, \"in_channels\": 3, \"out_channels\": 128,}, \n",
    "    \n",
    "    ],\n",
    "    [ \n",
    "        {\"nof_layers\": 2, \"in_channels\": 128, \"out_channels\": 256,}, \n",
    "    ],\n",
    "    [ \n",
    "        {\"nof_layers\": 2, \"in_channels\": 256, \"out_channels\": 512, }, \n",
    "    ],\n",
    "]\n",
    "\n",
    "conv_image_reduce_ratio = 2**len(model_stacked_conv_list)\n",
    "conv_final_image_width = input_image_width//conv_image_reduce_ratio\n",
    "conv_final_image_height = input_image_height//conv_image_reduce_ratio\n",
    "\n",
    "model_stacked_linear = [\n",
    "    { \"nof_layers\": 2, \"in_features\": conv_final_image_width * conv_final_image_height * 512, \"out_features\": 1024, \"dropout_p\": 0.5},\n",
    "    { \"nof_layers\": 1, \"in_features\": 1024, \"out_features\": 10, \"activation\": None}\n",
    "]\n",
    "\n",
    "model = VGG.VGG(\n",
    "    stacked_conv_descriptors = model_stacked_conv_list,\n",
    "    stacked_linear_descriptor = model_stacked_linear,\n",
    "    enable_debug = False,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf077d-469d-4814-afe2-d53baa3bcd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load pretraine model \n",
    "\n",
    "## NOTE: the pretrain_model_file_path needs to be set according to your pretrained model\n",
    "pretrain_model_file_path = r\".\\Results\\model_2024-05-13-12-33-13.pt\"\n",
    "\n",
    "pretain_model = torch.load(pretrain_model_file_path)\n",
    "\n",
    "print(pretain_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768e656-82ec-4078-8317-c50a2a6cecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update parameters from pretrained model to new model\n",
    "\n",
    "## NOTE: the model_from_pretrain_layer_mapping needs to be set according to your model achitecture\n",
    "model_from_pretrain_layer_mapping = [\n",
    "    [[0,0],[0,0]],\n",
    "    [[2,0],[2,0]],\n",
    "    [[4,0],[4,0]],\n",
    "    [[7,0],[7,0]],\n",
    "    [[7,6],[7,3]],\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i_item in range(len(model_from_pretrain_layer_mapping)):\n",
    "        cur_model_layers, cur_pretrain_layers = model_from_pretrain_layer_mapping[i_item]\n",
    "        # print(cur_model_layers, cur_pretrain_layers)\n",
    "        cur_pretrain_state_dict = None\n",
    "        if len(cur_pretrain_layers) == 1:\n",
    "            cur_pretrain_state_dict = pretain_model.network[cur_pretrain_layers[0]].state_dict()\n",
    "        elif len(cur_pretrain_layers) == 2:\n",
    "            cur_pretrain_state_dict = pretain_model.network[cur_pretrain_layers[0]].network[cur_pretrain_layers[1]].state_dict()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if len(cur_model_layers) == 1:\n",
    "            model.network[cur_model_layers[0]].load_state_dict(cur_pretrain_state_dict)\n",
    "        elif len(cur_model_layers) == 2:\n",
    "            model.network[cur_model_layers[0]].network[cur_model_layers[1]].load_state_dict(cur_pretrain_state_dict)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c39c3-a18c-497a-b38e-7eecf36df0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## quickly check if model can run\n",
    "\n",
    "model.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    check_features, check_labels = next(iter(train_dataloader))\n",
    "    model.eval()\n",
    "    print(model(check_features).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ec2a0-3a34-4628-b3f0-240029539fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training configuration\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1E-2, momentum = 0.9, weight_decay= 5E-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer = optimizer,\n",
    "    mode = \"max\",\n",
    "    factor = 0.1,\n",
    "    patience = 10,\n",
    "    threshold = 1E-3,\n",
    "    min_lr = 0,\n",
    ")\n",
    "\n",
    "top_k = 1\n",
    "\n",
    "validate_transforms = [None, torchvision.transforms.functional.hflip]\n",
    "\n",
    "nof_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9f06f-eeef-4503-812e-698c51fb06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use parallel computing if possible\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f449b-32fa-46cd-b77d-a0f2d68fae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training and validationg loop\n",
    "\n",
    "learning_rates = torch.zeros((nof_epochs,))\n",
    "train_losses = torch.zeros((nof_epochs,))\n",
    "validate_losses = torch.zeros((nof_epochs,))\n",
    "validate_accuracies = torch.zeros((nof_epochs,))\n",
    "\n",
    "for i_epoch in range(nof_epochs):\n",
    "    print(f\" ------ Epoch {i_epoch} ------ \")\n",
    "\n",
    "    cur_lr = optimizer.param_groups[0]['lr'];\n",
    "    \n",
    "    print(f\"current lr = {cur_lr}\")\n",
    "    learning_rates[i_epoch] = cur_lr\n",
    "\n",
    "    cur_train_loss = train_one_epoch(model, \n",
    "                                     train_dataloader,  \n",
    "                                     loss_func, \n",
    "                                     optimizer, \n",
    "                                     device)\n",
    "    cur_validate_loss, cur_validate_accuracy = validate_one_epoch_topk_aug(model, \n",
    "                                                                           validate_dataloader, \n",
    "                                                                           loss_func, \n",
    "                                                                           validate_transforms, \n",
    "                                                                           device, \n",
    "                                                                           top_k)\n",
    "    \n",
    "    train_losses[i_epoch] = cur_train_loss\n",
    "    validate_losses[i_epoch] =  cur_validate_loss\n",
    "    validate_accuracies[i_epoch] = cur_validate_accuracy\n",
    "\n",
    "    scheduler.step(cur_validate_accuracy)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5487f-7cb6-4bb2-ab56-b86c74bd7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation metrics\n",
    "plt.figure()\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(train_losses, label = \"train loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(validate_losses, label = \"validate loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(learning_rates, label = \"learning rate\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(validate_accuracies, label = \"accuracy\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e5a17-98c7-43e5-b144-466f38a440b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## validate for more top k calssification\n",
    "for i_topk in range(1, 6):    \n",
    "    cur_validate_loss, cur_validate_accuracy = validate_one_epoch_topk_aug(model, \n",
    "                                                                           validate_dataloader, \n",
    "                                                                           loss_func, \n",
    "                                                                           validate_transforms, \n",
    "                                                                           device, \n",
    "                                                                           i_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b9e1b-f3da-4b7e-b921-8d2a42a9b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model and model parameters\n",
    "\n",
    "dst_dir_path = r\".\\Results\"\n",
    "if not os.path.isdir(dst_dir_path):\n",
    "    os.makedirs(dst_dir_path)\n",
    "\n",
    "dst_model_name = \"model_\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "dst_model_file_name = dst_model_name + \".pt\"\n",
    "dst_modelstate_file_name = dst_model_name + \"_state.pt\"\n",
    "\n",
    "dst_model_file_path = os.path.join(dst_dir_path, dst_model_file_name)\n",
    "torch.save(model, dst_model_file_path)\n",
    "print(\"model saved to: \" + dst_model_file_path)\n",
    "\n",
    "dst_modelstate_file_path = os.path.join(dst_dir_path, dst_modelstate_file_name)\n",
    "torch.save(model.state_dict(), dst_modelstate_file_path)\n",
    "print(\"model state saved to: \" + dst_modelstate_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf054d-7ad3-4966-9743-9e814e354f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
