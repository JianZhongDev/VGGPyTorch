{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a8047-10e5-4502-8c01-7eaa242511ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import modules\n",
    "import os\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba95db-c06e-4718-bc7a-0edb0b4317a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import customized modules \n",
    "import Modules.Models.VGG as VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40e1e2-e3b8-4a40-85a4-6514c260d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demo creating 16-layer model\n",
    "\n",
    "input_image_width = 224\n",
    "input_image_height = 224\n",
    "\n",
    "model_stacked_conv_list = [\n",
    "    [ \n",
    "        {\"nof_layers\": 2, \"in_channels\": 3, \"out_channels\": 64,}, \n",
    "    \n",
    "    ],\n",
    "    [ \n",
    "        {\"nof_layers\": 2, \"in_ckjhannels\": 64, \"out_channels\": 128,}, \n",
    "    ],\n",
    "    [ \n",
    "        {\"nof_layers\": 2, \"in_channels\": 128, \"out_channels\": 256, },\n",
    "        {\"nof_layers\": 1, \"out_channels\": 256, \"kernel_size\": 1, \"padding\": 0},\n",
    "    ],\n",
    "    [ \n",
    "        {\"nof_layers\": 2, \"in_ckjhannels\": 256, \"out_channels\": 512,}, \n",
    "        {\"nof_layers\": 1, \"out_channels\": 512, \"kernel_size\": 1, \"padding\": 0},\n",
    "    ],\n",
    "    [ \n",
    "        {\"nof_layers\": 2, \"in_ckjhannels\": 512, \"out_channels\": 512,}, \n",
    "        {\"nof_layers\": 1, \"out_channels\": 512, \"kernel_size\": 1, \"padding\": 0},\n",
    "    ],\n",
    "]\n",
    "\n",
    "conv_image_reduce_ratio = 2**len(model_stacked_conv_list)\n",
    "conv_final_image_width = input_image_width//conv_image_reduce_ratio\n",
    "conv_final_image_height = input_image_height//conv_image_reduce_ratio\n",
    "\n",
    "model_stacked_linear = [\n",
    "    { \"nof_layers\": 2, \n",
    "     \"in_features\": conv_final_image_width * conv_final_image_height * 512, \n",
    "     \"out_features\": 4096, \n",
    "     \"dropout_p\": 0.5\n",
    "    },\n",
    "    { \"nof_layers\": 1, \n",
    "     \"out_features\": 1000, \n",
    "     \"activation\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "model = VGG.VGG(\n",
    "    stacked_conv_descriptors = model_stacked_conv_list,\n",
    "    stacked_linear_descriptor = model_stacked_linear,\n",
    "    enable_debug = False,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6002711-8e8c-46d1-a29a-3d7c5059784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demo transfer model parameters\n",
    "\n",
    "input_image_width = 32\n",
    "input_image_height = 32\n",
    "\n",
    "# create model 1\n",
    "model1_stacked_conv_list = [\n",
    "    [ \n",
    "        {\"nof_layers\": 1, \"in_channels\": 3, \"out_channels\": 64,}, \n",
    "    \n",
    "    ],\n",
    "    [ \n",
    "        {\"nof_layers\": 1, \"in_ckjhannels\": 64, \"out_channels\": 128,}, \n",
    "    ],\n",
    "]\n",
    "conv_image_reduce_ratio = 2**len(model1_stacked_conv_list)\n",
    "conv_final_image_width = input_image_width//conv_image_reduce_ratio\n",
    "conv_final_image_height = input_image_height//conv_image_reduce_ratio\n",
    "model1_stacked_linear = [\n",
    "    { \"nof_layers\": 1, \n",
    "     \"in_features\": conv_final_image_width * conv_final_image_height * 512, \n",
    "     \"out_features\": 512, \n",
    "     \"dropout_p\": 0.5\n",
    "    },\n",
    "    { \"nof_layers\": 1, \n",
    "     \"out_features\": 10, \n",
    "     \"activation\": None\n",
    "    }\n",
    "]\n",
    "model1 = VGG.VGG(\n",
    "    stacked_conv_descriptors = model1_stacked_conv_list,\n",
    "    stacked_linear_descriptor = model1_stacked_linear,\n",
    "    enable_debug = False,\n",
    ")\n",
    "\n",
    "# create model 2\n",
    "model2_stacked_conv_list = [\n",
    "    [ \n",
    "        {\"nof_layers\": 1, \"in_channels\": 3, \"out_channels\": 64,}, \n",
    "    \n",
    "    ],\n",
    "    [ \n",
    "        {\"nof_layers\": 2, \"in_ckjhannels\": 64, \"out_channels\": 128,}, \n",
    "    ],\n",
    "]\n",
    "conv_image_reduce_ratio = 2**len(model2_stacked_conv_list)\n",
    "conv_final_image_width = input_image_width//conv_image_reduce_ratio\n",
    "conv_final_image_height = input_image_height//conv_image_reduce_ratio\n",
    "model2_stacked_linear = [\n",
    "    { \"nof_layers\": 1, \n",
    "     \"in_features\": conv_final_image_width * conv_final_image_height * 512, \n",
    "     \"out_features\": 512, \n",
    "     \"dropout_p\": 0.5\n",
    "    },\n",
    "    { \"nof_layers\": 1, \n",
    "     \"out_features\": 10, \n",
    "     \"activation\": None\n",
    "    }\n",
    "]\n",
    "model2 = VGG.VGG(\n",
    "    stacked_conv_descriptors = model2_stacked_conv_list,\n",
    "    stacked_linear_descriptor = model2_stacked_linear,\n",
    "    enable_debug = False,\n",
    ")\n",
    "\n",
    "# transfer parameter of 1st convoluation layer from model 1 to model 2\n",
    "model2.network[0].network[0].load_state_dict(model1.network[0].network[0].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d8901-6405-4a17-a6fd-9710faf4b80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
